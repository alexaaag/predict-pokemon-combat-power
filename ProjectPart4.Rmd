---
title: "Part 4 | I Can't Bel-Eevee It's the End"
author: "Alex Gui and Lathan Liou"
date: "Date Submitted: May 9, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE, warning=FALSE, fig.height=3, fig.width=8, fig.align = "center",tidy.opts=list(width.cutoff=60),tidy=TRUE)
library(dplyr)
library(ggplot2)
library(skimr)
library(broom)
library(readr)
library(gridExtra)
library(GGally)
library(glmnet)
library(tidyr)
library(splines)
library(grid)
library(png)
library(pls)
options(digits=3)
pokemon <- read_csv("pokemon.csv")

pokemon_model1<-pokemon %>%
  dplyr::select(-name,-notes) %>%
  dplyr::select(-hp_new,-power_up_stardust_new,-attack_strong_value_new,-attack_weak_value_new,-weight_new,-height_new,-power_up_candy_new,-attack_strong_type_new,-attack_weak_type_new,-attack_strong_new,-attack_weak_new) %>%
  mutate(attack_strong_type = as.factor(attack_strong_type),
         attack_weak_type = as.numeric(as.factor(attack_weak_type)),
         attack_strong = as.numeric(as.factor(attack_strong)),
         attack_weak = as.numeric(as.factor(attack_weak)),
         species = as.numeric(as.factor(species)))

pokemon_logged <- pokemon_model1 %>%
  mutate(cp_new=log(cp_new),cp=log(cp))
names(pokemon_logged)[2] <- c("cp_log")
names(pokemon_logged)[14] <- c("cp_new_log")

pokemon_test <- pokemon %>%
  dplyr::select(-name,-notes) %>%
  dplyr::select(-hp_new,-power_up_stardust_new,-attack_strong_value_new,-attack_weak_value_new,-weight_new,-height_new,-power_up_candy_new,-attack_strong_type_new,-attack_weak_type_new,-attack_strong_new,-attack_weak_new)

pokemon_testlog <- pokemon_test %>%
  mutate(cp_new=log(cp_new),cp=log(cp))
names(pokemon_testlog)[2] <- c("cp_log")
names(pokemon_testlog)[14] <- c("cp_new_log")
```

\section*{Introduction}
In July 2016, Pokemon Go became an overnight sensation with hundreds of millions of people downloading the mobile game. For most players, this game represents a nice coffee break. However, for more serious players, the objective is to try to obtain the strongest Pokemon possible available, which is indicated by the highest combat power, abbreviated cp, so that you can battle other players' pokemon and win. Usually players can catch weaker forms of Pokemon that, through training, can evolve into stronger forms, so an evolved Pokemon will generally always have a higher cp than a non-evolved Pokemon. In the search for the strongest Pokemon, players have wished to determine what characteristics would indicate that the pokemon they have is stronger relative to other players' pokemon. For instance, if a player had pokemon X and trained it to its maximum potential but pokemon X was still weaker than pokemon Y, the player would want to know why pokemon Y was still stronger. A number of people have tried to generate models in an attempt to predict the best way to maximize cp for their Pokemon. This is what we will attempt to do ourselves: create a model to predict combat power for an evolved Pokemon. 

The dataset we are using to build our model is an original data set collected by $OpenIntro^1$. The dataset contains 75 observations across 26 variables, with each observation representing a randomly generated Pokemon that the gamer caught. Four species are represented in this data, so the conclusions drawn from this modeling process can only be inferred onto the population of these 4 particular species: Eevee, Pidgey, Caterpie, and Weedle. 

We avoid using "new" predictor variables in our modeling process because as a user, you wouldn't have access to any of the "new" information, but if you're interested in whether your pokemon will evolve into one with a high cp ($cp\_new$), you would want to know which of the Pokemon's current stats could indicate a high $cp\_new$. We also avoid categorical variables such as the name of the attack and attack type because they don't inherently contain any information that users would be concerned with; it's the attack value that matters. In our exploratory analysis, we also log-transformed cp and $cp\_new$ because we had noticed issues of nonconstant variance in the residual plot.

You can follow our work here: https://github.com/alexaaag/math158-project.

\section*{Ridge Regression and Lasso}
```{r, include=FALSE}
set.seed(312)

#create lambda grid
lambda.grid = 10^seq(5,-5, length =100)

#x matrix, categorical variables have been factorized and are numeric
x <- model.matrix( ~species + cp_log + hp + weight + height + power_up_stardust + power_up_candy + attack_weak_value + attack_strong_value, pokemon_logged)

#run CV RR
pokemon.rr.cv <- cv.glmnet(x, as.numeric(pokemon_logged$cp_new_log), alpha=0, lambda=lambda.grid, standardize = TRUE)
  
#find min lambda
opt_lambda_RR <- pokemon.rr.cv$lambda.min

#plot CVRR
plot(pokemon.rr.cv)
abline(v=log(pokemon.rr.cv$lambda.min), col="green")

#get RR coefficients
pokemon.rr.cv_fit <- pokemon.rr.cv$glmnet.fit
tidy(pokemon.rr.cv_fit)

#run CV lasso
pokemon.lasso.cv <- cv.glmnet(x, as.numeric(pokemon_logged$cp_new_log), alpha=1, lambda=lambda.grid, standardize = TRUE)

#find min lambda
opt_lambda_lasso <- pokemon.lasso.cv$lambda.min

#plot CVlasso
plot(pokemon.lasso.cv)
abline(v=log(pokemon.lasso.cv$lambda.min), col="green")

#get lasso coefficients
pokemon.lasso.cv_fit <- pokemon.lasso.cv$glmnet.fit
tidy(pokemon.lasso.cv_fit)


r2lasso <- pokemon.lasso.cv_fit$dev.ratio[which(pokemon.lasso.cv_fit$lambda == opt_lambda_lasso)]

MLR.model <- lm(cp_new_log ~ cp_log + species + attack_strong_value + 
    hp + weight, data = pokemon_logged)
summary(MLR.model)
```

We ran ridge regression and LASSO on our explanatory variables of interest. 

```{r, echo=FALSE}
par(mfrow=c(1,2))
#plot CVRR
RRplot <- plot(pokemon.rr.cv)
abline(v=log(pokemon.rr.cv$lambda.min), col="green")

#plot CVlasso
lassoplot <- plot(pokemon.lasso.cv)
abline(v=log(pokemon.lasso.cv$lambda.min), col="green")
dev.off()

coef.MLR <- coef(MLR.model)[-1]
coef.MLR1 <- c(-0.000808, 1.186140, -0.009644, -0.018610, 0, 0, 0, 0, 0.020735)
coef.RR <- as.numeric(coef(pokemon.rr.cv, s = opt_lambda_RR))[-c(1,2)]
coef.lasso <- as.numeric(coef(pokemon.lasso.cv, s = opt_lambda_lasso))[-c(1,2)]
pairs(data.frame(coef.MLR1, coef.RR, coef.lasso), pch = 19, cex = .75, col = "black")
```

As shown in the pairs plot, ridge regression shrunk the coefficients closer to zero than multiple linear regression did. On the other hand, lasso regression not only shrunk the coefficients but also performed variable selection more selectively than our stepwise regression did previously, selecting $cp_{log}$, $attack\_strong\_value$ and $attack\_weak\_value$ at our optimal lambda value.

```{r, include=FALSE}
#predicted response from MLR
MLR.model<-lm(cp_new_log ~ cp_log + species + attack_strong_value + 
    hp + weight, data = pokemon_logged)
val.rr <- data.frame(pokemon_logged[,-10])
pred.MLR <- augment(MLR.model,newdata=val.rr,type.predict = "response")
pred.MLR$.fitted

#predicted response from RR
pred.RR <- predict(pokemon.rr.cv_fit, newx = x, s = opt_lambda_RR)

#predicted response from lasso
pred.lasso <- predict(pokemon.lasso.cv_fit, newx = x, s = opt_lambda_lasso)

#tidy data
pokemon_pred <- data.frame(pokemon_logged$cp_new_log, pred.MLR$.fitted, pred.RR, pred.lasso)
colnames(pokemon_pred) <- c("cp_new_obs", "cp_new_MLR", "cp_new_RR", "cp_new_lasso")
pokemon_tidy <- gather(pokemon_pred, key = Method, value = Prediction, -cp_new_obs)
```

Next, we wanted to see how the predictions from our multiple linear regression, ridge regression, and our LASSO models would fare against each other.

```{r, echo=FALSE}
#plot predictions
ggplot(pokemon_tidy, aes(x = cp_new_obs, y = Prediction, group = Method, color = Method)) +
  geom_jitter() + 
  geom_smooth(method = 'lm', se = FALSE)
```

From this figure, it seems ridge regression and lasso seem to predict very similarly as multiple linear regression, since the slopes of each regression fit basically overlap each other. 

\section*{Smoothing}
```{r, include=FALSE}
xlims <- range(pokemon_logged$cp_log)
x.grid <- seq(from=xlims[1], to=xlims[2])

#regression spline with df=3
cpnew.rs1 <- lm(cp_new_log ~ bs(cp_log, df=3, degree = 1), data = pokemon_logged)
SSE.rs1 <- sum(cpnew.rs1$residuals^2)

cpnew.rs1.pred <- predict(cpnew.rs1, newdata = list(cp_log=x.grid), se = TRUE)
cpnew.rs1.se <- cbind(cpnew.rs1.pred$fit + 2*cpnew.rs1.pred$se.fit,
                   cpnew.rs1.pred$fit - 2*cpnew.rs1.pred$se.fit)

plot(pokemon_logged$cp_log, pokemon_logged$cp_new_log, xlim = range(pokemon_logged$cp_log), cex = .5, pch = 19, 
     col = "darkgrey", xlab = "Cp_log", ylab = "Cp_new_log")
title("Regression Spline (df=3), SSE=4.429", outer = F)
lines(x.grid, cpnew.rs1.pred$fit, lwd = 2, col = "blue")
matlines(x.grid, cpnew.rs1.se, lwd = 1, col = "blue", lty = 3)

#regression spline with df=4
cpnew.rs2 <- lm(cp_new_log ~ bs(cp_log, df=4, degree = 1), data = pokemon_logged)
SSE.rs2 <- sum(cpnew.rs2$residuals^2)

cpnew.rs2.pred <- predict(cpnew.rs2, newdata = list(cp_log=x.grid), se = TRUE)
cpnew.rs2.se <- cbind(cpnew.rs2.pred$fit + 2*cpnew.rs2.pred$se.fit,
                   cpnew.rs2.pred$fit - 2*cpnew.rs2.pred$se.fit)

plot(pokemon_logged$cp_log, pokemon_logged$cp_new_log, xlim = range(pokemon_logged$cp_log), cex = .5, pch = 19, 
     col = "darkgrey", xlab = "Cp_log", ylab = "Cp_new_log")
title("Regression Spline (df=4), SSE=4.335", outer = F)
lines(x.grid, cpnew.rs2.pred$fit, lwd = 2, col = "blue")
matlines(x.grid, cpnew.rs2.se, lwd = 1, col = "blue", lty = 3)

#regression spline with df=5
cpnew.rs3 <- lm(cp_new_log ~ bs(cp_log, df=5, degree = 1), data = pokemon_logged)
SSE.rs3 <- sum(cpnew.rs3$residuals^2)

cpnew.rs3.pred <- predict(cpnew.rs3, newdata = list(cp_log=x.grid), se = TRUE)
cpnew.rs3.se <- cbind(cpnew.rs3.pred$fit + 2*cpnew.rs3.pred$se.fit,
                   cpnew.rs3.pred$fit - 2*cpnew.rs3.pred$se.fit)

plot(pokemon_logged$cp_log, pokemon_logged$cp_new_log, xlim = range(pokemon_logged$cp_log), cex = .5, pch = 19, 
     col = "darkgrey", xlab = "Cp_log", ylab = "Cp_new_log")
title("Regression Spline (df=5), SSE=4.300", outer = F)
lines(x.grid, cpnew.rs3.pred$fit, lwd = 2, col = "blue")
matlines(x.grid, cpnew.rs3.se, lwd = 1, col = "blue", lty = 3)

#regression spline with df=6
cpnew.rs4 <- lm(cp_new_log ~ bs(cp_log, df=6, degree = 1), data = pokemon_logged)
SSE.rs4 <- sum(cpnew.rs4$residuals^2)

cpnew.rs4.pred <- predict(cpnew.rs4, newdata = list(cp_log=x.grid), se = TRUE)
cpnew.rs4.se <- cbind(cpnew.rs4.pred$fit + 2*cpnew.rs4.pred$se.fit,
                   cpnew.rs4.pred$fit - 2*cpnew.rs4.pred$se.fit)

plot(pokemon_logged$cp_log, pokemon_logged$cp_new_log, xlim = range(pokemon_logged$cp_log), cex = .5, pch = 19, 
     col = "darkgrey", xlab = "Cp_log", ylab = "Cp_new_log")
title("Regression Spline (df=6), SSE=4.322", outer = F)
lines(x.grid, cpnew.rs4.pred$fit, lwd = 2, col = "blue")
matlines(x.grid, cpnew.rs4.se, lwd = 1, col = "blue", lty = 3)

#plot all the splines on one
splineplot <- plot(pokemon_logged$cp_log, pokemon_logged$cp_new_log, xlim = range(pokemon_logged$cp_log), cex = .5, pch = 19, col = "darkgrey", xlab = "Cp_log", ylab = "Cp_new_log")
title("Regression Spline", outer = F)
lines(x.grid, cpnew.rs1.pred$fit, lwd = 2, col = "blue")
matlines(x.grid, cpnew.rs1.se, lwd = 1, col = "blue", lty = 3)
lines(x.grid, cpnew.rs2.pred$fit, lwd = 2, col = "green")
matlines(x.grid, cpnew.rs2.se, lwd = 1, col = "green", lty = 3)
lines(x.grid, cpnew.rs3.pred$fit, lwd = 2, col = "red")
matlines(x.grid, cpnew.rs3.se, lwd = 1, col = "red", lty = 3)
lines(x.grid, cpnew.rs4.pred$fit, lwd = 2, col = "black")
matlines(x.grid, cpnew.rs4.se, lwd = 1, col = "black", lty = 3)
```

```{r, include=FALSE}
#loess, span 0.2
cpnew.loess1 <- loess(cp_new_log ~ cp_log, span = 0.2, data = pokemon_logged)

cpnew.loess1.pred <- predict(cpnew.loess1, newdata = data.frame(cp_log=x.grid), se = TRUE)
cpnew.loess1.se <- cbind(cpnew.loess1.pred$fit + 2*cpnew.loess1.pred$se.fit,
                    cpnew.loess1.pred$fit - 2*cpnew.loess1.pred$se.fit)
SSE.loess1 <- sum(cpnew.loess1$residuals^2)

#loess, span 0.4
cpnew.loess2 <- loess(cp_new_log ~ cp_log, span = 0.4, data = pokemon_logged)

cpnew.loess2.pred <- predict(cpnew.loess2, newdata = data.frame(cp_log=x.grid), se = TRUE)
cpnew.loess2.se <- cbind(cpnew.loess2.pred$fit + 2*cpnew.loess2.pred$se.fit,
                    cpnew.loess2.pred$fit - 2*cpnew.loess2.pred$se.fit)
SSE.loess2 <- sum(cpnew.loess2$residuals^2)

#loess, span 0.6
cpnew.loess3 <- loess(cp_new_log ~ cp_log, span = 0.6, data = pokemon_logged)

cpnew.loess3.pred <- predict(cpnew.loess3, newdata = data.frame(cp_log=x.grid), se = TRUE)
cpnew.loess3.se <- cbind(cpnew.loess3.pred$fit + 2*cpnew.loess3.pred$se.fit,
                    cpnew.loess3.pred$fit - 2*cpnew.loess3.pred$se.fit)
SSE.loess3 <- sum(cpnew.loess3$residuals^2)

#loess, span 0.8
cpnew.loess4 <- loess(cp_new_log ~ cp_log, span = 0.8, data = pokemon_logged)

cpnew.loess4.pred <- predict(cpnew.loess4, newdata = data.frame(cp_log=x.grid), se = TRUE)
cpnew.loess4.se <- cbind(cpnew.loess4.pred$fit + 2*cpnew.loess4.pred$se.fit,
                    cpnew.loess4.pred$fit - 2*cpnew.loess4.pred$se.fit)

#loess plot
loessplot <- plot(pokemon_logged$cp_log, pokemon_logged$cp_new_log, xlim = xlims, cex = .5, pch = 19, col = "darkgrey", xlab = "cp_log", ylab = "cp_new_log")
title("Loess Fit", outer = F)
lines(x.grid, cpnew.loess1.pred$fit, lwd = 2, col = "blue")
matlines(x.grid, cpnew.loess1.se, lwd = 1, col = "blue", lty = 3)
lines(x.grid, cpnew.loess2.pred$fit, lwd = 2, col = "green")
matlines(x.grid, cpnew.loess2.se, lwd = 1, col = "green", lty = 3)
lines(x.grid, cpnew.loess3.pred$fit, lwd = 2, col = "red")
matlines(x.grid, cpnew.loess3.se, lwd = 1, col = "red", lty = 3)
lines(x.grid, cpnew.loess4.pred$fit, lwd = 2, col = "black")
matlines(x.grid, cpnew.loess4.se, lwd = 1, col = "black", lty = 3)
```

```{r, echo=FALSE}
par(mfrow=c(1,2))

splineplot <- plot(pokemon_logged$cp_log, pokemon_logged$cp_new_log, xlim = range(pokemon_logged$cp_log), cex = .5, pch = 19, col = "darkgrey", xlab = "Cp_log", ylab = "Cp_new_log")
title("Regression Spline", outer = F)
lines(x.grid, cpnew.rs1.pred$fit, lwd = 2, col = "blue")
matlines(x.grid, cpnew.rs1.se, lwd = 1, col = "blue", lty = 3)
lines(x.grid, cpnew.rs2.pred$fit, lwd = 2, col = "green")
matlines(x.grid, cpnew.rs2.se, lwd = 1, col = "green", lty = 3)
lines(x.grid, cpnew.rs3.pred$fit, lwd = 2, col = "red")
matlines(x.grid, cpnew.rs3.se, lwd = 1, col = "red", lty = 3)
lines(x.grid, cpnew.rs4.pred$fit, lwd = 2, col = "black")
matlines(x.grid, cpnew.rs4.se, lwd = 1, col = "black", lty = 3)

loessplot <- plot(pokemon_logged$cp_log, pokemon_logged$cp_new_log, xlim = xlims, cex = .5, pch = 19, col = "darkgrey", xlab = "cp_log", ylab = "cp_new_log")
title("Loess Fit", outer = F)
lines(x.grid, cpnew.loess1.pred$fit, lwd = 2, col = "blue")
matlines(x.grid, cpnew.loess1.se, lwd = 1, col = "blue", lty = 3)
lines(x.grid, cpnew.loess2.pred$fit, lwd = 2, col = "green")
matlines(x.grid, cpnew.loess2.se, lwd = 1, col = "green", lty = 3)
lines(x.grid, cpnew.loess3.pred$fit, lwd = 2, col = "red")
matlines(x.grid, cpnew.loess3.se, lwd = 1, col = "red", lty = 3)
lines(x.grid, cpnew.loess4.pred$fit, lwd = 2, col = "black")
matlines(x.grid, cpnew.loess4.se, lwd = 1, col = "black", lty = 3)
```

We chose $cp_{log} to run our smoothing spline and the loess regressions. The smoothing splines and loess fit the data extremely well. Changing the degrees of freedom, and hence the number of knots, for the smoothing splines improves the fit minimally. At a certain point, increasing the degrees of freedom actually begins to increase SSE. Increasing the span from 0.2 for loess actually increases SSE. 

The spline and loess models are all extremely similar due to the nature of the data being perfectly correlated, but I would choose the spline model since it fits the data very smoothly, and it still has a functional form which lends itself to interpretability. 

\subsection*{Conclusion}

Overall, running ridge regression, lasso, and smoothing methods such as regression splines and loess did not improve the model fit relative to multiple linear regression by very much. This is fairly unsurprising to us because we noticed how extremely well linear regression fit our dataset previously, and this can most likely be attributed to the nature of how $cp_{new}$ is actually modeled in the game. We think $cp_{new}$ is likely coded into the game as a function of a linear combination of certain predictors and our multiple linear regression model fairly closely matches the real model used in-game. 

\section*{Something New}

\subsection*{Q-Q Plot}

A normal probability plot is used to identify substantial departures from normality in the data. We chose in particular to plot what is known as a normal quantile-quantile plot (Q-Q plot for short), which plots sample quantiles ordered and plotted in a continuous cumulative distribution function against theoretical quantiles from a standard normal distribution. A $y=x$ reference line is also plotted and if the sample data also come from a normal distribution, the points should fall roughly along this reference line. A q-q plot is important because it can provide more information about whether the normality assumption is violated.

```{r, echo=FALSE}
pokemon.stdres = rstandard(MLR.model)
qqnorm(pokemon.stdres,
       ylab = "Sample Quantiles",
       xlab="Theoretical Quantiles",
       main="Normal Q-Q Plot")
qqline(pokemon.stdres)

#qqPlot(MLR.model)
#qqnorm(pokemon_logged$cp_log)
#qqline(pokemon_logged$cp_log, col = "steelblue", lwd = 2)
```

In our Q-Q plot, we observe that the outlying points do not fall on the line; however, we are not concerned with our ability to do inference since we did note a couple of outliers from before, and the majority of the data is fit by the model.   

\subsection*{Added Variable Plots}

Added variable plots (AV plots), also known as partial regression plot, attempt to show the marginal effect of adding another variable to a model already having one or more independent variables. Added variable plots are formed by first computing the residuals of regressing the response variable against the independent variable(s) but omitting the variable of interest, $X_i$. Let's call this $Y._{[i]}$ Next, the residuals are computed from regressing $X_i$ against the remaining independent variables. Let's call this $X_{i.[i]}$ The residuals from $Y._{[i]}$ and $X_{i.[i]}$ are then plotted against each other. For this analysis, one underlying assumption is that the explanatory variables are not highly correlated with each other and that the explanatory variables have to be quantitative. One way to interpret AV plots is to compare the scatter of the points about the least squares lines and the scatter of the points around the horizontal line at 0. If the scatters are different, then we conclude that adding variable X to the model substantially reduced the error sum of squares. 

```{r, echo=FALSE, fig.width=8, fig.height=8}
library(car)
avPlots(MLR.model)
```

As shown, variables such as $cp\_log$ and $attack\_strong\_value$ are highly correlated with $cp\_new\_log$, which indicate that adding $cp\_log$ or $attack\_strong\_value$ substantially reduces the error sum of squares. On the other hand, variables such as weight were slightly correlated with $cp\_new\_log$, indicating that adding weight to the regression model does not substantially reduce the error sum of squares. In fact, the coefficient of partial determination for the linear effect of weight is $R^2_{Y weight|cp, species, attack\_strong\_value, hp}=0.0283$. One thing to note is that the AV plot does not really make sense for species since species is a factor variable. 

Another benefit of an added variable plot is it allows us to determine influential points, after accounting for the other variables in the model.

\section*{Principal Components Analysis (PCA)}
Principal components analysis is widely used as an unsupervised learning method for feature extraction and data compression. In our analysis, we will apply principal components in our regression model as a dimensionality reduction technique. The intuition behind PCA is: given a set of highly correlated predictors, PCA will transform it into a smaller set of linearly independent variables called principal components. The transformation is defined such that the first principal component direction captures the greatest possible variability in the data, in others words, explain the most variability of the data. The succeeding principal components are linear combinations of the variables that is un- correlated with the preceding component and has largest variance subject to this constraint. The set of components constitute a basis for our data space.

\subsection{Principal Components Regression}
```{r,echo=FALSE}
pokemondata<-pokemon %>%
  dplyr::select(-notes,-name,-attack_weak_type,-attack_strong_type,-attack_strong_type_new,-attack_weak,-attack_strong,-attack_weak_type_new,-attack_weak_new,-attack_strong_new) %>%
  mutate(species=ifelse(species=="Pidgey",1,ifelse(species=="Weedle",2,ifelse(species=="Caterpie",3,4)))) %>%
  mutate(cpnew=cp_new)

pokemondata<-pokemondata[,!grepl("_new", colnames(pokemondata))]
```

The principal components regression approach will first construct $M$ principal components and then regress on the components instead of individual predictors. The underlying assumption of the model is "the directions in which $X_1,...X_p$ shows the greatest variance are those associated with Y". Although this assumption is not guaranteed, it regardless provides a decent approximation that often yields good results. $M$, the number of principal components, is our tuning parameter that will be chosen by cross-validation.

We believe PCA works well with our pokemon data given the existence of strong correlation among our predictors. Our analysis shows that PCR greatly reduced variance and contributed to our model's predictive power.

```{r,fig.width=8,fig.height=5,include=FALSE}
res<-cor(pokemondata)
library(corrplot)
corrplot(res, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)
title(sub="Correlation Plot")
```

## Principal Components
Our model first constructs 9 principal components(this makes sense since $p=9$ and $M \leq p$). 
```{r,echo=FALSE}
pokemonpca<-pokemondata[,!grepl("new", colnames(pokemondata))]

pca <- prcomp(pokemonpca,scale. = TRUE)
```

To visualize it:

```{r,echo=FALSE,fig.width=5,fig.height=5,include=FALSE}
biplot(pca,scale=1)
```

Look at PC1. We observe that the higher the performance metrics, the higher the PC1 value. Therefore we can interpret PC1 as a measurement of overall strength. As for PC2, we notice that higher PC2 is associated with higher attack value. Therefore we interpret PC2 as a measurement of attack strength.

We then plot our pokemon on our new principal components space:

```{r,fig.width=5,fig.height=5,echo=FALSE}
library(png)

# Get the PCA data
pd <- cbind.data.frame(pokemondata, pca$x)

#change back to specie names
pd$species <- as.character(pd$species)
pd$species[pd$species == "1"] <- "Pidgey"
pd$species[pd$species == "2"] <- "Weedle"
pd$species[pd$species == "3"] <- "Caterpie"
pd$species[pd$species == "4"] <- "Eevee"

# A function to plot Pokemon's png file as ggplot2's annotation_custom
f_annotate <- function(x, y, name, size) {
  f_getImage <- function(name) {
    rasterGrob(readPNG(paste0("pokemon_png/", name, ".png")))
  }
  annotation_custom(f_getImage(name),
                    xmin = x - size, xmax = x + size, 
                    ymin = y - size, ymax = y + size)
}

# Wrap everything in a plot function
f_plot <- function(pd) {
  ggplot(data = pd, aes(x = PC1, y = PC2)) +
    geom_text(data = pd, aes(label = species), 
              hjust = 1.5, vjust = -1, size = 1.5, alpha = 0.5) +
    mapply(f_annotate, x = pd$PC1, y = pd$PC2, name = pd$species, size = 0.8)  +
    theme_bw() +
    labs(x = "Overall Strength (PC1)", y = "Attack Strength (PC2)") +
    coord_fixed(xlim = c(-5, 5), ylim = c(-4, 4))
}


f_plot(pd)
```

\textbf{Interesting Insights:} Using principal components, we get to create two new powerful metrics to evaluate our pokemons. From the plot, you can observe that Eeeve in general has high overall strength and high attack strength. Pidgey has good attack stats but is weaker in general due to the creature's intrinsic nature. Caterpie and Weedle are weak on both metrics. Overall, this PCA gives you a high-level overview of our pokemon's strength. You should feel excited!

## Regression: 

```{r,echo=FALSE}
pr.var=pca$sdev^2
pve=pr.var/sum(pr.var)
plot(pve, xlab="Principal Component", ylab="Proportion of Variance Explained ", ylim=c(0,1),type='l')
```
Observe that the first two principal components explain nearly 60% of the variability in the data. As $M \to 10$, the marginal contribution to variability explained decreases. Our regression model will use cross validation to tune $M$, the number of components as predictors.

The cross-validation uses $root\_mean\_squared\_error$ as the metric. $M=9$ returns the smallest cv score. 
```{r,include=FALSE}
library(pls)
set.seed(47)
#train=sample(1:nrow(pokemondata),nrow(pokemondata)*2/3)
pcr.fit = pcr(cpnew~.,data=pokemondata,scale=TRUE,validation="CV")
summary(pcr.fit)
```

```{r,echo=FALSE}
validationplot(pcr.fit,val.type="RMSE")
```

Therefore we used all the components to build the linear regression model. Our result shows that all regression coefficients are signigicant and the adjusted $R^2=0.983$. 
```{r,include=FALSE}
#pcr.pred=predict(pcr.fit,pokemondata[-train,],ncomp=2) 
#mean((pcr.pred-pokemondata[-train,]$cpnew)^2)
pc1 = pca$x[,1] 
pc2 = pca$x[,2]
pc3 = pca$x[,3]
pc4 = pca$x[,4]
pc5 = pca$x[,5]
pc6 = pca$x[,6]
pc7 = pca$x[,7]
pc8 = pca$x[,8]
pc9 = pca$x[,9]
summary(lm(pokemondata$cpnew~pc1+pc2+pc3+pc4+pc5+pc6+pc7+pc8+pc9))
```

Interpretation:

Our principal component regression model performs really well in predicting $cp_new$. We also observe that our top two principal components (overall strength, attack strength) correspond to the predictors selected by LASSO (cp,attack_strong and attak_weak). However, we decide not to go with PCR model because it is not interpretable. 

\section*{Summary: How to get the best Pokemon?}
Are you a Pokemon Go player? Have you been struggling with trying to identify the strong pokemon and winning your battles? Are you curious if your pokemon will be good enough after evolution?

These are the questions our report attempts to address. To kick start, we ran a comprehensive overview of the data at hand.
```{r,fig.height=8,fig.width=13,echo=FALSE}
library(GGally)
pokemon2<-pokemon %>%
  dplyr::select(cp_new,cp,species,hp,weight,attack_strong_value,power_up_stardust)
ggpairs(pokemon2, aes(col=species))
```

We observed that our response variable $cp\_new$ is strongly correlated with $cp$, indicating that $cp$ might be a promising predictor. We also noticed the existence of correlation between pairs of explnatory variables and we kept that in mind in our model building process. 

After substantial analysis, we decided to present a linear regression model using three of the most important features that will help users predict their pokemon's combat power post-evolution. To reiterate, since our data only includes four species, our inference only applies to the population of these four species. We used LASSO (Least Absolute Shrinkage and Selection Operator) to build our model. The decision is motivated by 1) high $R^2$ 2) high interpretability 3) our residual plot and 4) correspondence with our PCA analysis that complements the LASSO model. The LASSO model works well with our data because our variables are predominantly quantitative and the model's feature selection capacity helps users understand what the characteristics are of a strong pokemon.

Our model is: $E[\log_{cp\_new}]=0.64+0.89\log{cp}+0.012attack\_weak+0.0081attack\_strong$. The cross-validated $R^2$ is 0.984 which means the model explains 98.4% of the variability in $\log(cp\_new)$. This model corresponds with our conclusion from $PCA$ whose top two principal components indicate overall strength ($cp$) and attack strength ($attack\_strong$ and $attack\_weak$). Essentially, this means that what users should be primarily concerned with when determining whether their Pokemon is inherently strong or not is their base combat power prior to evolution as well its move strength. The residual plot further shows that 1) linearity is met and 2) there is a good scatter around the 0 line indicating constant variance. We do have influential points that correspond to the Eeeves but we decided not to remove them because we believe our users are equally interested in the performance of Eeeves as well. Overall, the technical conditions are met and we are comfortable with inference on the following four Pokemon: Caterpie, Weedle, Pidgey and Eevee.

```{r,echo=FALSE}
plot(x=pred.lasso,y=pred.lasso-pokemon_logged$cp_new_log,xlab="fitted",ylab="residual",main="residual plot")
abline(h=0,col="red")
```

```{r,include=FALSE}
pokemon2<-pokemon %>%
  dplyr::select(cp_new,cp,species,hp,weight,attack_strong_value,power_up_stardust)
```

\section*{Future Direction}
Overall, the most interesting insight that surprised us was the fact that $cp_{new}$ could be explained primarily by cp, or combat power prior to evolution and attack value (both strong and weak). We did not expect this given that our exploratory data analysis did not indicate that attack value was highly correlated with $cp_{new}$. If we had more data of different species of Pokemon we would have liked to explore whether cp and species alone can predict $cp_{new}$ since in our correlation plot above, there is almost a perfect correlation between cp and $cp_{new}$. Further, having more data on differen Pokemon species would have allowed our analysis to be inferred onto many many more Pokemon that a Pokemon Go player might want to know about. 
We also wish that there was data on battle statistics as well such as combat damage dealt to see whether that could factor in to explaining a Pokemon's strength. 


